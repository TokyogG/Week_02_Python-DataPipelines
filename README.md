### **Modern Python Vectorization & Data Pipeline Benchmarks**

*Part of the 16-Week Edge AI Engineering Bootcamp*

---

## üéØ **Goal**

Understand and benchmark the performance differences between:

* **Pure Python loops**
* **NumPy** (C + SIMD)
* **Polars** (Rust + Apache Arrow + multithreading)
* **PyTorch** (tensor engine)

Tasks:

1. **Elementwise multiply** on arrays of size **N = 50,000,000**
2. **Realistic workload**: Group sensor readings by `sensor_id` and compute mean
   (5,000,000 rows, 1,000 sensor IDs)

This establishes why vectorized, columnar, and tensor-based operations are essential for **edge data pipelines**.

---
### Repo Structure

```bash
Week_02_Python-DataPipelines/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ sensors/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ mpu6050.py          # Thin wrapper around MPU6050 driver
‚îÇ   ‚îî‚îÄ‚îÄ pipelines/
‚îÇ       ‚îî‚îÄ‚îÄ parquet_writer.py   # Chunked Parquet writer using Polars
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îî‚îÄ‚îÄ parquet/                # Logged IMU chunks (Day 2)
‚îú‚îÄ‚îÄ logger_v2.py                # 20 Hz streaming logger ‚Üí Parquet
‚îú‚îÄ‚îÄ live_dashboard.py           # Basic Parquet-backed dashboard
‚îú‚îÄ‚îÄ live_dashboard_pro.py       # Direct-streaming PRO demo (dark mode, 3D cube)
‚îî‚îÄ‚îÄ README.md

---

# üß™ **A. Elementwise Multiply (N = 50,000,000)**

| Method        | Time (sec) | Speed-up vs Python |
| ------------- | ---------- | ------------------ |
| Python (loop) | **1.9672** | 1.0√ó               |
| NumPy         | **0.1124** | 17.5√ó              |
| Polars        | **0.1187** | 16.6√ó              |
| PyTorch       | **0.0703** | 28.0√ó              |

### üîç **Interpretation**

* **Python loops** scale linearly and quickly saturate CPU.
* **NumPy** and **PyTorch** leverage **contiguous memory + SIMD**, making them drastically faster.
* **Polars**, although a DataFrame engine, catches up at large N (the DataFrame overhead becomes negligible).
* **PyTorch** is fastest because it uses highly optimized tensor kernels.

**Conclusion:**
For raw numeric transforms on large arrays, NumPy/Polars/PyTorch deliver **16‚Äì28√ó speedups** over Python.

---

# üß™ **B. Realistic Task ‚Äî GroupBy Mean (5,000,000 sensor readings)**

| Method                      | Time (sec) | Speed-up vs Python |
| --------------------------- | ---------- | ------------------ |
| Python groupby (dict-based) | **1.4242** | 1.0√ó               |
| Polars groupby              | **0.1375** | 10.4√ó              |

### üîç **Interpretation**

* Pure Python requires manual dictionary accumulation ‚Äî **slow, single-threaded**, no SIMD.
* **Polars** uses:

  * Rust
  * Apache Arrow columnar memory
  * Multithreading (Rayon)
  * SIMD vectorization
* It completes the same task **10.4√ó faster**, even with only 1,000 group keys.

This is where Polars shines: realistic analytics workloads, not just toy elementwise ops.

---

# üß† **Why This Matters for Edge AI Pipelines**

Your workload in the Edge AI Bootcamp (and in your railway sensor projects) involves:

* Large, continuous sensor streams
* Timestamp alignment
* Grouping / averaging / window filters
* Multi-sensor joins
* Parquet I/O
* Post-processing for ML and quantized inference

These are EXACTLY the operations where Polars is engineered to win.

**This benchmark demonstrates:**

| Task Type                               | Best Tool       | Why                        |
| --------------------------------------- | --------------- | -------------------------- |
| pure math on huge arrays                | PyTorch / NumPy | optimal SIMD kernels       |
| dataframe analytics, groupby, windowing | Polars          | Rust engine, multithreaded |
| Python loops                            | ‚ùå never use     | no SIMD, slow              |

---

# üìÅ **Files in This Folder**

```
Week_02/
  Day_01/
    benchmarks.py     # Runs all benchmarks end-to-end
    README.md         # (this file)
```

---

# ‚ñ∂Ô∏è **How to Run**

```
python benchmarks.py
```

Runs all tests:

* Elementwise vectorization benchmarks
* Realistic sensor-grouping benchmark

---

# üèÅ **Summary**

* Python loops: **too slow for edge workloads**
* NumPy: **strong baseline**, SIMD-enabled
* PyTorch: **fastest for raw tensor ops**
* Polars: **best for real multi-column analytics**, 10√ó faster groupbys
* For your edge pipelines, Polars + PyTorch form the ideal foundation.

---

## Day 2 ‚Äì Real-Time Sensor Pipeline + Dashboards


**Goal:** Turn a simple IMU logger into a modern, efficient data pipeline with live visualization.

### What I Built

- **Streaming logger (`logger_v2.py`)**
  - Reads MPU6050 IMU over I¬≤C at **20 Hz**
  - Buffers samples in memory and writes **chunked Parquet** files (`data/parquet/`)
  - Uses **Polars** for fast DataFrame handling
  - Designed to keep RAM and CPU usage low on a Raspberry Pi 5

- **Student dashboard (`live_dashboard.py`)**
  - Reads latest Parquet chunk(s) with Polars
  - Dash + Plotly app with:
    - Live plot of `accel_x` (and optionally other axes)
    - Rolling window of recent samples
  - Updates every 0.5 s (Dash `Interval` callback)

- **Pro dashboard (`live_dashboard_pro.py`) ‚Äì demo only**
  - **No disk I/O**: talks directly to the MPU6050 in a background thread
  - Keeps a rolling **10 second ring buffer** in RAM (deque)
  - Dark-mode Dash UI with:
    - KPI cards:
      - RMS Accel (m/s¬≤)
      - Peak Gyro (¬∞/s)
      - Temperature (¬∞C)
      - Motion state: `STILL / MOVING / SHAKING`
    - 3 stacked time-series plots:
      - Acceleration (x, y, z)
      - Gyro (x, y, z)
      - Temperature
    - 3D **orientation cube** driven by accelerometer-based pitch/roll
  - Runs at ~5‚Äì10 FPS, fully interactive in the browser

### How to Run

#### 1. Logger + basic dashboard (student version)

```bash
# Terminal 1 ‚Äì logger
cd ~/EdgeAI_Bootcamp/Week_02_Python-DataPipelines
python logger_v2.py

# Terminal 2 ‚Äì basic dashboard
cd ~/EdgeAI_Bootcamp/Week_02_Python-DataPipelines
python live_dashboard.py

# If running Pro Dashboard there is not need to run logger but note that no parquet files are kept
# It's more of a showcase of what can be presented

cd ~/EdgeAI_Bootcamp/Week_02_Python-DataPipelines
python live_dashboard_pro.py

## Day 3 Quantization performance results

### TinyNet ‚Äì Dynamic Quantization (PyTorch)

| Model        | Type   | Avg Latency (ms) | Speedup |
|-------------|--------|------------------|---------|
| TinyNet     | FP32   | 0.2805           | 1.00√ó   |
| TinyNet     | INT8   | 0.2285           | 1.23√ó   |

On a tiny fully-connected network, dynamic INT8 quantization gives ~20% speedup on the Pi 5 CPU. Larger models (e.g. MobileNet) and batched inputs should show bigger gains

### BiggerNet ‚Äì Dynamic Quantization (PyTorch)

| Model        | Type   | Avg Latency (ms) | Speedup  |
|--------------|--------|------------------|----------|
| BiggerNet    | FP32   | 1.3467           | 1.00√ó    |
| BiggerNet    | INT8   | 0.4190           | **3.21√ó**|

Quantizing a larger FC model produces a 3.2√ó speedup on the Pi 5. This matches expected INT8 gains and confirms that quantization benefits grow significantly with model size

## üî¢ Quantization Methods ‚Äî Comparison Table

| Method | What Gets Quantized | Accuracy | Speedup | Memory Reduction | Calibration Needed | Best For |
|--------|----------------------|----------|---------|------------------|---------------------|----------|
| **Dynamic Quantization** | Weights (INT8), activations stay FP32 | ‚≠ê‚≠ê‚òÜ‚òÜ (Moderate) | ‚≠ê‚≠ê‚òÜ‚òÜ (~1.2‚Äì2√ó) | ‚≠ê‚≠ê‚òÜ‚òÜ (‚âà4√ó smaller weights) | ‚ùå No | LLMs, Transformers on CPU, fast prototyping |
| **Static PTQ (Post-Training Quantization)** | Weights + activations (INT8) | ‚≠ê‚≠ê‚≠ê‚òÜ (Good) | ‚≠ê‚≠ê‚≠ê‚òÜ (~1.5‚Äì3√ó) | ‚≠ê‚≠ê‚≠ê‚≠ê (4√ó smaller model + smaller activations) | ‚úîÔ∏è Yes (small calibration dataset) | CNNs, MobileNet, ResNet, image models |
| **QAT (Quantization-Aware Training)** | Weights + activations (INT8 simulated during training) | ‚≠ê‚≠ê‚≠ê‚≠ê (Best) | ‚≠ê‚≠ê‚≠ê‚≠ê (~2‚Äì4√ó) | ‚≠ê‚≠ê‚≠ê‚≠ê (4√ó smaller) | ‚úîÔ∏è Yes (training/fine-tuning) | Production-grade edge AI, MCUs, NPUs, tiny models |
| **INT4 Quantization** | Weights (INT4) + optional activations | ‚≠ê‚≠ê‚òÜ‚òÜ to ‚≠ê‚≠ê‚≠ê‚òÜ | ‚≠ê‚≠ê‚≠ê‚≠ê (~3‚Äì5√ó) | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (8√ó smaller) | Depends (dynamic or PTQ) | LLMs on CPU/GPU, memory-constrained models |
| **Mixed-Precision (FP16 + INT8)** | Critical layers FP16, others INT8 | ‚≠ê‚≠ê‚≠ê‚≠ê (Very high) | ‚≠ê‚≠ê‚≠ê‚òÜ (~1.5‚Äì2.5√ó) | ‚≠ê‚≠ê‚≠ê‚òÜ (2√ó‚Äì3√ó) | Optional | Models that lose too much accuracy in full INT8 |

### Notes
- **QAT provides the best accuracy** and is preferred for edge deployments (Pi, Hailo, NPUs, MCUs).
- **PTQ requires a small ‚Äúcalibration set‚Äù** (100‚Äì1000 samples) to map activation ranges.
- **Dynamic quantization is the easiest** but gives the smallest gains.
- **INT4** is becoming the standard for LLMs when memory is tight.
- **Speedups vary by hardware** (ARM CPUs ‚âà 1.3‚Äì3√ó, NPUs ‚âà 5‚Äì10√ó).
- **Per-channel INT8** for weights produces significantly better accuracy than per-tensor.
- **Symmetric weights, asymmetric activations** is the industry standard layout.

### Recommended Quantization Path for Edge AI
1. **Start with Dynamic Quantization** ‚Üí quick size/latency check.
2. **Move to Static PTQ** ‚Üí get INT8 activations + lower latency.
3. **Finish with QAT** ‚Üí maximize accuracy for deployment to Pi/Hailo/MCU.

