{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c536f2b",
   "metadata": {},
   "source": [
    "# Week 2 Day 4 – MobileNetV2 INT8 Quantization (Instructor Version)\n",
    "\n",
    "This notebook benchmarks **FP32 vs INT8 MobileNetV2** on CPU (Raspberry Pi 5 or similar).\n",
    "- Uses TorchVision pretrained + quantized models.\n",
    "- Reports model size and latency.\n",
    "- Designed to be clean and easy to adapt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decd9792",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import psutil\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "print(\"PyTorch:\", torch.__version__)\n",
    "print(\"Torchvision:\", getattr(models, '__version__', 'N/A'))\n",
    "print(\"CUDA available:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd554200",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "def measure_latency(model: nn.Module, example_input: torch.Tensor, \n",
    "                    n_warmup: int = 10, n_iters: int = 50) -> float:\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    example_input = example_input.to(device)\n",
    "    \n",
    "    # Warmup\n",
    "    with torch.no_grad():\n",
    "        for _ in range(n_warmup):\n",
    "            _ = model(example_input)\n",
    "    \n",
    "    start = time.perf_counter()\n",
    "    with torch.no_grad():\n",
    "        for _ in range(n_iters):\n",
    "            _ = model(example_input)\n",
    "    end = time.perf_counter()\n",
    "    \n",
    "    return (end - start) * 1000.0 / n_iters  # ms\n",
    "\n",
    "\n",
    "def get_model_size_mb(model: nn.Module, filename: str) -> float:\n",
    "    torch.save(model.state_dict(), filename)\n",
    "    size_mb = os.path.getsize(filename) / (1024 * 1024)\n",
    "    return size_mb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12aabae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FP32 MobileNetV2\n",
    "try:\n",
    "    fp32_model = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.IMAGENET1K_V1)\n",
    "except Exception:\n",
    "    fp32_model = models.mobilenet_v2(pretrained=True)\n",
    "\n",
    "fp32_model.eval()\n",
    "fp32_model.to(device)\n",
    "\n",
    "example_input = torch.randn(1, 3, 224, 224)\n",
    "\n",
    "# Sanity check\n",
    "with torch.no_grad():\n",
    "    _ = fp32_model(example_input.to(device))\n",
    "\n",
    "process = psutil.Process(os.getpid())\n",
    "\n",
    "fp32_size = get_model_size_mb(fp32_model, \"mobilenet_v2_fp32.pth\")\n",
    "fp32_latency = measure_latency(fp32_model, example_input, n_warmup=5, n_iters=20)\n",
    "fp32_rss = process.memory_info().rss / (1024 * 1024)\n",
    "\n",
    "print(f\"FP32 size:    {fp32_size:.2f} MB\")\n",
    "print(f\"FP32 latency: {fp32_latency:.2f} ms\")\n",
    "print(f\"FP32 RSS:     {fp32_rss:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6a99dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INT8 MobileNetV2 (quantized)\n",
    "try:\n",
    "    from torchvision.models.quantization import mobilenet_v2 as q_mobilenet_v2\n",
    "    \n",
    "    int8_model = q_mobilenet_v2(pretrained=True, quantize=True)\n",
    "    int8_model.eval()\n",
    "    int8_model.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        _ = int8_model(example_input.to(device))\n",
    "    \n",
    "    process = psutil.Process(os.getpid())\n",
    "    \n",
    "    int8_size = get_model_size_mb(int8_model, \"mobilenet_v2_int8.pth\")\n",
    "    int8_latency = measure_latency(int8_model, example_input, n_warmup=5, n_iters=20)\n",
    "    int8_rss = process.memory_info().rss / (1024 * 1024)\n",
    "    \n",
    "    print(f\"INT8 size:    {int8_size:.2f} MB\")\n",
    "    print(f\"INT8 latency: {int8_latency:.2f} ms\")\n",
    "    print(f\"INT8 RSS:     {int8_rss:.2f} MB\")\n",
    "    \n",
    "    speedup = fp32_latency / int8_latency if int8_latency > 0 else float('inf')\n",
    "    print(f\"Speed-up vs FP32: {speedup:.2f}x\")\n",
    "except Exception as e:\n",
    "    print(\"Failed to load quantized MobileNetV2:\", e)\n",
    "    int8_model = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2a95d2",
   "metadata": {},
   "source": [
    "## Summary (for README)\n",
    "\n",
    "Use the printed values above to populate a small table like:\n",
    "\n",
    "| Model        | Precision | Size (MB) | Latency (ms) | Speed-up vs FP32 |\n",
    "|--------------|-----------|-----------|--------------|------------------|\n",
    "| MobileNetV2  | FP32      | …         | …            | 1.0x             |\n",
    "| MobileNetV2  | INT8      | …         | …            | …                |\n",
    "\n",
    "This matches the **Week 2 Day 4** goal: demonstrate that quantization reduces model size and can improve latency on CPU-only edge devices.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
