{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "febb0c20",
   "metadata": {},
   "source": [
    "# Week 2 Day 4 â€“ MobileNetV2 INT8 Quantization (Student Version)\n",
    "\n",
    "**Goal for today:**\n",
    "\n",
    "- Load a pretrained **MobileNetV2** model in PyTorch (FP32).\n",
    "- Measure **model size** and **inference latency** on CPU.\n",
    "- Load a **quantized INT8 MobileNetV2**.\n",
    "- Compare FP32 vs INT8 in a simple results table.\n",
    "- Reflect on what quantization does and why it matters for Edge AI.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f54987",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports\n",
    "\n",
    "If you're on the Raspberry Pi for the first time with PyTorch, you may need to install `torch` and `torchvision` first (this can take a while and is usually done once).\n",
    "\n",
    "> ðŸ’¡ **Note:** If `torchvision` is already installed, you can skip any install commands.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12543c20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.1+cpu\n",
      "CUDA available?: False\n"
     ]
    }
   ],
   "source": [
    "# If needed, uncomment and run this cell ONCE to install PyTorch/torchvision on Pi.\n",
    "# On Pi, you typically use the prebuilt wheels from PyTorch's site.\n",
    "# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n",
    "\n",
    "import os\n",
    "import time\n",
    "import psutil\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available?:\", torch.cuda.is_available())  # Should be False on Pi (CPU-only)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b32621b",
   "metadata": {},
   "source": [
    "## 2. Device & Helper Functions\n",
    "\n",
    "We will force everything to run on **CPU**, since the Raspberry Pi 5 is a CPU-based edge device (plus your Hailo accelerator later).\n",
    "\n",
    "We'll also define a small helper to measure:\n",
    "\n",
    "- **Latency (ms)** over several runs\n",
    "- **Model size (MB)** when saved to disk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b14cbdfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "def measure_latency(model: nn.Module, example_input: torch.Tensor, \n",
    "                    n_warmup: int = 10, n_iters: int = 50) -> float:\n",
    "    \"\"\"Measure average inference latency in milliseconds.\"\"\"\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    example_input = example_input.to(device)\n",
    "    \n",
    "    # Warmup (not measured)\n",
    "    with torch.no_grad():\n",
    "        for _ in range(n_warmup):\n",
    "            _ = model(example_input)\n",
    "    \n",
    "    # Timed runs\n",
    "    start = time.perf_counter()\n",
    "    with torch.no_grad():\n",
    "        for _ in range(n_iters):\n",
    "            _ = model(example_input)\n",
    "    end = time.perf_counter()\n",
    "    \n",
    "    avg_latency_ms = (end - start) * 1000.0 / n_iters\n",
    "    return avg_latency_ms\n",
    "\n",
    "\n",
    "def get_model_size_mb(model: nn.Module, filename: str = \"temp_model.pth\") -> float:\n",
    "    \"\"\"Save model to disk and report file size in MB.\"\"\"\n",
    "    torch.save(model.state_dict(), filename)\n",
    "    size_mb = os.path.getsize(filename) / (1024 * 1024)\n",
    "    # Clean up file if you like\n",
    "    # os.remove(filename)\n",
    "    return size_mb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2118ab0b",
   "metadata": {},
   "source": [
    "## 3. Load Pretrained FP32 MobileNetV2\n",
    "\n",
    "We start with the **standard FP32 MobileNetV2**.\n",
    "\n",
    "> ðŸ“¦ This will download pretrained weights the first time you run it (requires internet).  \n",
    "> After that, it will load from cache.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0fc3bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/mobilenet_v2-b0353104.pth\" to /home/tokyog/.cache/torch/hub/checkpoints/mobilenet_v2-b0353104.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13.6M/13.6M [00:00<00:00, 27.0MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded MobileNetV2 with explicit weights enum.\n",
      "Output shape: torch.Size([1, 1000])\n"
     ]
    }
   ],
   "source": [
    "# Newer torchvision versions use arguments like:\n",
    "# models.mobilenet_v2(weights=models.MobileNet_V2_Weights.IMAGENET1K_V1)\n",
    "# To stay compatible with more versions, we try both.\n",
    "\n",
    "try:\n",
    "    fp32_model = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.IMAGENET1K_V1)\n",
    "    print(\"Loaded MobileNetV2 with explicit weights enum.\")\n",
    "except Exception as e:\n",
    "    print(\"Falling back to older API:\", e)\n",
    "    fp32_model = models.mobilenet_v2(pretrained=True)\n",
    "\n",
    "fp32_model.eval()\n",
    "fp32_model.to(device)\n",
    "\n",
    "# Create a dummy input tensor: 1 image, 3 channels, 224x224\n",
    "example_input = torch.randn(1, 3, 224, 224)\n",
    "\n",
    "# Quick shape check\n",
    "with torch.no_grad():\n",
    "    out = fp32_model(example_input.to(device))\n",
    "print(\"Output shape:\", out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36121ca9",
   "metadata": {},
   "source": [
    "## 4. Benchmark FP32 MobileNetV2\n",
    "\n",
    "Here we measure:\n",
    "\n",
    "- **Model size (MB)**  \n",
    "- **Average latency (ms)** on CPU  \n",
    "- **RSS Memory usage (MB)** for the current Python process (rough estimate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6c81c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP32 MobileNetV2 size: 13.60 MB\n",
      "FP32 MobileNetV2 avg latency: 81.83 ms\n",
      "Approx. RSS memory (process): 464.80 MB\n"
     ]
    }
   ],
   "source": [
    "process = psutil.Process(os.getpid())\n",
    "\n",
    "fp32_size_mb = get_model_size_mb(fp32_model, filename=\"mobilenet_v2_fp32.pth\")\n",
    "fp32_latency_ms = measure_latency(fp32_model, example_input, n_warmup=5, n_iters=20)\n",
    "\n",
    "fp32_rss_mb = process.memory_info().rss / (1024 * 1024)\n",
    "\n",
    "print(f\"FP32 MobileNetV2 size: {fp32_size_mb:.2f} MB\")\n",
    "print(f\"FP32 MobileNetV2 avg latency: {fp32_latency_ms:.2f} ms\")\n",
    "print(f\"Approx. RSS memory (process): {fp32_rss_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096a8e24",
   "metadata": {},
   "source": [
    "### âœï¸ Exercise: Record Your FP32 Numbers\n",
    "\n",
    "Write down your results:\n",
    "\n",
    "- FP32 model size (MB):\n",
    "- FP32 latency (ms):\n",
    "- FP32 RSS memory (MB):\n",
    "\n",
    "You'll compare them with the quantized model next.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9d4597",
   "metadata": {},
   "source": [
    "## 5. Load Quantized INT8 MobileNetV2\n",
    "\n",
    "TorchVision provides a **quantized version of MobileNetV2** that has already been prepared and quantized using `torch.quantization` under the hood.\n",
    "\n",
    "This is a **post-training static quantization** example.\n",
    "\n",
    "> âš ï¸ If this import fails, your `torchvision` might not support quantization models.  \n",
    "> In that case, ask your instructor or check the PyTorch/torchvision version.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "214711a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tokyog/edge_bootcamp/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/tokyog/edge_bootcamp/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_QuantizedWeights.IMAGENET1K_QNNPACK_V1`. You can also use `weights=MobileNet_V2_QuantizedWeights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/home/tokyog/edge_bootcamp/lib/python3.11/site-packages/torch/ao/quantization/utils.py:435: UserWarning: must run observer before calling calculate_qparams. Returning default values.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/quantized/mobilenet_v2_qnnpack_37f702c5.pth\" to /home/tokyog/.cache/torch/hub/checkpoints/mobilenet_v2_qnnpack_37f702c5.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.42M/3.42M [00:00<00:00, 16.9MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded quantized INT8 MobileNetV2 from torchvision.models.quantization.\n",
      "Quantized output shape: torch.Size([1, 1000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/tokyog/edge_bootcamp/lib/python3.11/site-packages/torch/_utils.py:444: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  device=storage.device,\n"
     ]
    }
   ],
   "source": [
    "# Quantized models live in a separate module in many torchvision versions\n",
    "try:\n",
    "    from torchvision.models.quantization import mobilenet_v2 as q_mobilenet_v2\n",
    "\n",
    "    quantized_model = q_mobilenet_v2(pretrained=True, quantize=True)\n",
    "    print(\"Loaded quantized INT8 MobileNetV2 from torchvision.models.quantization.\")\n",
    "    \n",
    "    quantized_model.eval()\n",
    "    quantized_model.to(device)\n",
    "    \n",
    "    # Quick forward to verify\n",
    "    with torch.no_grad():\n",
    "        out_q = quantized_model(example_input.to(device))\n",
    "    print(\"Quantized output shape:\", out_q.shape)\n",
    "except Exception as e:\n",
    "    print(\"âŒ Could not import quantized MobileNetV2:\", e)\n",
    "    print(\"You may need a newer torchvision, or a different approach to quantization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905f36a8",
   "metadata": {},
   "source": [
    "## 6. Benchmark Quantized INT8 MobileNetV2\n",
    "\n",
    "We repeat the same measurements:\n",
    "\n",
    "- Model size (MB)\n",
    "- Latency (ms)\n",
    "- RSS memory (MB)\n",
    "\n",
    "> If the previous cell failed (import error), skip this or fix your environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38c123fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INT8 MobileNetV2 size: 3.46 MB\n",
      "INT8 MobileNetV2 avg latency: 18.09 ms\n",
      "Approx. RSS memory (process): 527.11 MB\n"
     ]
    }
   ],
   "source": [
    "if 'quantized_model' in globals():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    \n",
    "    int8_size_mb = get_model_size_mb(quantized_model, filename=\"mobilenet_v2_int8.pth\")\n",
    "    int8_latency_ms = measure_latency(quantized_model, example_input, n_warmup=5, n_iters=20)\n",
    "    \n",
    "    int8_rss_mb = process.memory_info().rss / (1024 * 1024)\n",
    "    \n",
    "    print(f\"INT8 MobileNetV2 size: {int8_size_mb:.2f} MB\")\n",
    "    print(f\"INT8 MobileNetV2 avg latency: {int8_latency_ms:.2f} ms\")\n",
    "    print(f\"Approx. RSS memory (process): {int8_rss_mb:.2f} MB\")\n",
    "else:\n",
    "    print(\"Quantized model is not available in this environment.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142e8c29",
   "metadata": {},
   "source": [
    "## 7. Compare Results\n",
    "\n",
    "Fill in this table with your actual numbers from the outputs above:\n",
    "\n",
    "| Model | Precision | Size (MB) | Latency (ms) | Speed-up vs FP32 | Notes |\n",
    "|-------|-----------|-----------|--------------|------------------|-------|\n",
    "| MobileNetV2 | FP32 | â¬œ | â¬œ | â€” | Baseline |\n",
    "| MobileNetV2 | INT8 | â¬œ | â¬œ | â¬œ | Smaller & faster? |\n",
    "\n",
    "> ðŸ’¡ **Speed-up vs FP32** = `FP32_latency / INT8_latency`  \n",
    "> Example: if FP32 = 40 ms and INT8 = 20 ms, speed-up = 2x.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf113a52",
   "metadata": {},
   "source": [
    "## 8. Reflection Questions ðŸ§ \n",
    "\n",
    "Answer these in your own words (you can add a new Markdown cell below):\n",
    "\n",
    "1. **Why does INT8 quantization usually reduce file size?**  \n",
    "2. **How did latency change after quantization on your Raspberry Pi?**  \n",
    "3. **If there is a small accuracy drop with INT8, why might that still be acceptable for edge devices?**  \n",
    "4. **How could quantization help when deploying models on very small MCUs or when power usage is critical?**  \n",
    "\n",
    "You can also try:\n",
    "\n",
    "- Changing the number of iterations in `measure_latency`.\n",
    "- Using a **larger batch size** in `example_input` (e.g., batch=4).\n",
    "- Swapping MobileNetV2 for another model (e.g., ResNet18) and repeating the experiment.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edge_bootcamp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
